{
  "items": [

    {
      "item_id":1,
      "item_name": "Binary Search Algorithm",
      "item_summary": "A search algorithm to find the position of a target value within a sorted array",
      "item_description": "Binary search, (also known as half-interval search, logarithmic search, or binary chop) is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.\n\nBinary search runs in logarithmic time in the worst case, making {\\displaystyle O(\\log n)}O(\\log n) comparisons, where {\\displaystyle n}n is the number of elements in the array, the {\\displaystyle O}O is Big O notation, and {\\displaystyle \\log }\\log  is the logarithm.[6] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.\n\nBinary search works on sorted arrays. Binary search begins by comparing an element in the middle of the array with the target value. If the target value matches the element, its position in the array is returned. If the target value is less than the element, the search continues in the lower half of the array. If the target value is greater than the element, the search continues in the upper half of the array. By doing this, the algorithm eliminates the half in which the target value cannot lie in each iteration.\n\nIn 1946, John Mauchly made the first mention of binary search as part of the Moore School Lectures, a seminal and foundational college course in computing. In 1957, William Wesley Peterson published the first method for interpolation search. Every published binary search algorithm worked only for arrays whose length is one less than a power of two until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays.In 1962, Hermann Bottenbruch presented an ALGOL 60 implementation of binary search that placed the comparison for equality at the end, increasing the average number of iterations by one, but reducing to one the number of comparisons per iteration. The uniform binary search was developed by A. K. Chandra of Stanford University in 1971. In 1986, Bernard Chazelle and Leonidas J. Guibas introduced fractional cascading as a method to solve numerous search problems in computational geometry.",
      "item_image": "img_binary_search",
      "item_favourite": true,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Binary_search_algorithm",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://simple.wikipedia.org/wiki/File:Binary_search_into_array_-_example.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 2,
      "item_name": "Quick Sort Algorithm",
      "item_summary": "An efficient sorting algorithm",
      "item_description":"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm. Developed by British computer scientist Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.\n\nQuicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.\n\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.\n\nMathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.\n\nQuicksort is a divide and conquer algorithm. It first divides the input array into two smaller sub-arrays: the low elements and the high elements. It then recursively sorts the sub-arrays.",
      "item_image": "img_quick_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Quicksort",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Quicksort-iteration.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 3,
      "item_name": "Insertion Sort Algorithm",
      "item_summary": "A sorting algorithm that builds the final sorted array one item at a time",
      "item_description": "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages with having simple implementation, being efficient for small data sets and being stable. When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\n\nInsertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\n\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\n\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort.",
      "item_image": "img_insertion_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Insertion_sort",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Insertion-sort.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 4,
      "item_name": "Selection Sort Algorithm",
      "item_summary": "An in-place comparison sorting algorithm. ",
      "item_description": "Selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations.\n\nThe algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.\n\nThe time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort. One thing which distinguishes selection sort from other sorting algorithms is that it makes the minimum possible number of swaps, n − 1 in the worst case.\n\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of Θ(n2)), selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the kth iteration, the first k elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the k + 1st element, while selection sort must scan all remaining elements to find the k + 1st element.",
      "item_image": "img_selection_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Selection_sort",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://upload.wikimedia.org/wikipedia/commons/6/67/SelectionSort.jpg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 5,
      "item_name": "Bubble Sort Algorithm",
      "item_summary": "A simple sorting algorithm that repeatedly steps through the list",
      "item_description": "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements \"bubble\" to the top of the list.\n\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as merge sort are used by the sorting libraries built into popular programming languages.\n\nBubble sort has a worst-case and average complexity of О(n2), where n is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often O(n log n). Even other О(n2) sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. Therefore, bubble sort is not a practical sorting algorithm.\n\nThe only significant advantage that bubble sort has over most other algorithms, even quicksort, but not insertion sort, is that the ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only O(n). By contrast, most other algorithms, even those with better average-case complexity, perform their entire sorting process on the set and thus are more complex. However, not only does insertion sort share this advantage, but it also performs better on a list that is substantially sorted (having a small number of inversions).\n\nBubble sort should be avoided in the case of large collections. It will not be efficient in the case of a reverse-ordered collection.",
      "item_image": "img_bubble_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Bubble_sort",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:BubbleSort.jpg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 6,
      "item_name": "Merge Sort Algorithm",
      "item_summary": "An efficient general-purpose comparison-based sorting algorithm",
      "item_description": "Merge sort is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945.\n\nA merge sort works by dividing the unsorted list into n sublists, each containing one element (a list of one element is considered sorted) then repeatedly merging sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list. In sorting n objects, merge sort has an average and worst-case performance of O(n log n). \n\nMerge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort. Merge sort's most common implementation does not sort in place. Therefore, the memory size of the input must be allocated for the sorted output to be stored in.\n\nAlthough heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(n). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.",
      "item_image": "img_merge_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia ",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Merge_sort",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Merge_sort_algorithm_diagram.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 7,
      "item_name": "Breadth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level. It uses the opposite strategy as depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes.\n\nBFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, but this was not published until 1972. It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze and later developed by C. Y. Lee into a wire routing algorithm which was published in 1961.\n\nBFS uses a queue and it checks whether a vertex has been discovered before enqueueing the vertex rather than delaying this check until the vertex is dequeued from the queue. The queue contains the frontier along which the algorithm is currently searching. Nodes can be labelled as discovered by storing them in a set, or by an attribute on each node, depending on the implementation. Note that the word node is usually interchangeable with the word vertex. The parent attribute of each node is useful for accessing the nodes in a shortest path, for example by backtracking from the destination node up to the starting node, once the BFS has been run, and the predecessors nodes have been set. Breadth-first search produces a so-called breadth first tree.\n\nBreadth-first search can be used to solve many problems in graph theory such as finding the shortest path between two nodes with path length measured by number of edges.",
      "item_image": "img_breadth_first_search",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Breadth-first_search",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Breadth-first_tree.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 8,
      "item_name": "Depth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.\n\nFor applications of DFS in relation to specific domains, such as searching for solutions in artificial intelligence or web-crawling, the graph to be traversed is often either too large to visit in its entirety or infinite. DFS may suffer from non-termination. In such cases, search is only performed to a limited depth; due to limited resources, such as memory or disk space, one typically does not use data structures to keep track of the set of all previously visited vertices. When an appropriate depth limit is not known a priori, iterative deepening depth-first search applies DFS repeatedly with a sequence of increasing limits. \n\nIn the artificial intelligence mode of analysis, with a branching factor greater than one, iterative deepening increases the running time by only a constant factor over the case in which the correct depth limit is known due to the geometric growth of the number of nodes per level.",
      "item_image": "img_depth_first_search",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Depth-first_search",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Depth-first-tree.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 9,
      "item_name": "Heap Sort Algorithm",
      "item_summary": "A comparison-based sorting algorithm",
      "item_description": "Heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step. Heapsort was invented by J. W. J. Williams in 1964.[2] This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort. \n\nThe heapsort algorithm can be divided into two parts. In the first step, a heap is built out of the data (see Binary heap § Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nThe Heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm. Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O(n2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. Thus, because of the O(n log n) upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort. Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort such as being a stable sort.",
      "item_image": "img_heap_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Heapsort",
      "item_image_source": "Wikimedia ",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Heap_sort_algorithm-phase1.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 10,
      "item_name": "Kruskal's Algorithm",
      "item_summary": "A greedy algorithm to find a minimum spanning tree for a connected weighted graph.",
      "item_description": "Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. A Minimum Spanning Tree is a spanning tree of a connected, undirected graph. It connects all vertices together with the minimal weighting for its edges.\n\nThis is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component). This algorithm was written by Joseph Kruskal and first appeared in in 1956.",
      "item_image": "img_kruskals_algorithm",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Kruskal's_algorithm",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Kruskal_Algorithm_1.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 11,
      "item_name": "Dijkstra's Algorithm",
      "item_summary": "An algorithm for finding the shortest paths between nodes in a graph",
      "item_description": "Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\n\nThe algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.\n\nFor a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithm is network routing protocols.\n\nDijkstra's algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.",
      "item_image": "img_dijkstras",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Dijkstra_algorithm_example_27.svg",
      "item_group": "Algorithm"
    },

    {
      "item_id": 12,
      "item_name": "Stack",
      "item_summary": "A collection of elements wşth push and pop operations.",
      "item_description": "A stack is an abstract data type that serves as a collection of elements, with two principal operations. These operataions are push, which adds an element to the collection, and pop, which removes the most recently added element that was not yet removed.\n\nThe order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack.[1] The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.\n\nConsidered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search.\n\nKlaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea of a stack in 1955 and filed a patent in 1957.",
      "item_image": "img_stack",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Stack_(abstract_data_type)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Lifo_stack.png",
      "item_group": "Data Structure"
    },

    {
      "item_id": 13,
      "item_name": "Queue",
      "item_summary": "A linear collection of objects that are inserted and removed according to the FIFO principle. ",
      "item_description": "A queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and removal from the other end of the sequence. By convention, the end of the sequence at which elements are added is called the back, tail, or rear of the queue and the end at which elements are removed is called the head or front of the queue, analogously to the words used when people line up to wait for goods or services.\n\nThe operation of adding an element to the rear of the queue is known as enqueue, and the operation of removing an element from the front is known as dequeue. Other operations may also be allowed, often including a peek or front operation that returns the value of the next element to be dequeued without dequeuing it.\n\nThe operations of a queue make it a first-in-first-out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. A queue is an example of a linear data structure, or more abstractly a sequential collection. Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes.",
      "item_image": "img_queue",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Queue_(abstract_data_type)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Data_Queue.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 14,
      "item_name": "Graph",
      "item_summary": "A non-linear data structure consisting of nodes and edges",
      "item_description": "A graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from the field of graph theory within mathematics.\n\nA graph data structure consists of a finite (and possibly mutable) set of vertices (also called nodes or points), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as edges (also called links or lines), and for a directed graph are also known as arrows. The vertices may be part of the graph structure, or may be external entities represented by integer indices or references. A graph data structure may also associate to each edge some edge value, such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).",
      "item_image": "img_graph",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Graph_(abstract_data_type)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Directed.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 15,
      "item_name": "Heap",
      "item_summary": "A specialized tree-based data structure",
      "item_description": "A heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.[2] The node at the \"top\" of the heap (with no parents) is called the root node.\n\nThe heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as \"heaps\", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority.\n\nA common implementation of a heap is the binary heap, in which the tree is a binary tree. The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm.",
      "item_image": "img_heap",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Heap_(data_structure)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Max-Heap.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 16,
      "item_name": "Linked List",
      "item_summary": "A linear data structure consists of nodes where each node contains a data field and a link to the next node",
      "item_description": "A linked list is a linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence. In its most basic form, each node contains: data, and a reference (in other words, a link) to the next node in the sequence. This structure allows for efficient insertion or removal of elements from any position in the sequence during iteration. A drawback of linked lists is that access time is linear. Faster access, such as random access, is not feasible. Arrays have better cache locality compared to linked lists. Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation.\n\nLinked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis.\n\nThe principal benefit of a linked list over a conventional array is that the list elements can be easily inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while restructuring an array at run-time is a much more expensive operation. Linked lists allow insertion and removal of nodes at any point in the list, and allow doing so with a constant number of operations by keeping the link previous to the link being added or removed in memory during list traversal.\n\nOn the other hand, since simple linked lists by themselves do not allow random access to the data or any form of efficient indexing, many basic operations—such as obtaining the last node of the list, finding a node that contains a given datum, or locating the place where a new node should be inserted—may require iterating through most or all of the list elements. Linked list are dynamic, so the length of list can increase or decrease as necessary.",
      "item_image": "img_linked_list",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Linked_list",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Singly-linked-list.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 17,
      "item_name": "Hash Table",
      "item_summary": "A data structure which stores data in an array format with unique index value",
      "item_description": "A hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found.\n\nIn many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.\n\nThe idea of hashing is to distribute the entries (key/value pairs) across an array of buckets. Given a key, the algorithm computes an index that suggests where the entry can be found.\n\nA basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests.\n\nThe main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance. Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small.",
      "item_image": "img_hash_table",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Hash_table",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Hash_table_3_1_1_0_1_0_0_SP.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 18,
      "item_name": "Tree",
      "item_summary": "A nonlinear data structure",
      "item_description": "A tree is a widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.\n\nA tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node.\n\nA tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees.",
      "item_image": "img_tree",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Tree_(data_structure)",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Tree_(computer_science).svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 19,
      "item_name": "Associative Array",
      "item_summary": "An abstract data type composed of a collection of key, value pairs",
      "item_description": "An associative array (also called map, symbol table, or dictionary) is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection. Operations associated with this data type allow the addition of a pair to the collection, the removal of a pair from the collection, the modification of an existing pair, the lookup of a value associated with a particular key.\n\nAssociative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern. In an associative array, the association between a key and a value is often known as a \"mapping\", and the same word mapping may also be used to refer to the process of creating a new association.",
      "item_image": "img_associative_array",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Associative_array",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/Associative_array",
      "item_group": "Data Structure"
    },

    {
      "item_id": 20,
      "item_name": "Array",
      "item_summary": "a data structure consisting of a collection of elements",
      "item_description": "An array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index. The simplest type of data structure is a linear array, also called one-dimensional array.\n\nBecause the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word table is sometimes used as a synonym of array.\n\nArrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses.\n\nArrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation.\n\nArrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.\n\nArrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient, requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).",
      "item_image": "img_array",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Array_data_structure",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Array_of_array_storage.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 21,
      "item_name": "Vector",
      "item_summary": "An array with a dynamic size",
      "item_description": "Vector is generally a one-dimensional array, typically storing numbers. Unlike static arrays, which are always of a fixed size, vectors can be grown.This can be done either explicitly or by adding more data. In order to do this efficiently, the typical vector implementation grows by doubling its allocated space (rather than incrementing it) and often has more space allocated to it at any one time than it needs. This is because reallocating memory is usually an expensive operation. Therefore vectors use a dynamically allocated array to store their elements. \n\nThe vector data structure can be used to represent the mathematical vector used in linear algebra. Vectors are often used in computing in computer graphics and simulating physical systems.",
      "item_image": "img_array",
      "item_favourite": false,
      "item_text_source": "Wikiuniversity",
      "item_text_source_url":"https://en.wikiversity.org/wiki/Data_Structures_and_Algorithms/Arrays,_Lists_and_Vectors",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Array_of_array_storage.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 22,
      "item_name": "Dynamic Array",
      "item_summary": "A variable-size list data structure that allows elements to be added or removed",
      "item_description": "A dynamic array (also called growable array, resizable array, dynamic table, mutable array or array list) is a random access, variable-size list data structure that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages. Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation.\n\nA dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end.\n\nA simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. \n\nElements can be added at the end of a dynamic array in constant time by using the reserved space, until this space is completely consumed. When all space is consumed, and an additional element is to be added, then the underlying fixed-sized array needs to be increased in size. Typically resizing is expensive because it involves allocating a new underlying array and copying each element from the original array. Elements can be removed from the end of a dynamic array in constant time, as no resizing is required. The number of elements used by the dynamic array contents is its logical size or size, while the size of the underlying array is called the dynamic array's capacity or physical size, which is the maximum possible size without relocating data.\n\nA dynamic array might be preferred if the maximum logical size is unknown, or difficult to calculate, before the array is allocated, it is considered that a maximum logical size given by a specification is likely to change and the amortized cost of resizing a dynamic array does not significantly affect performance or responsiveness.",
      "item_image": "img_dynamic_array",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Dynamic_array",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Dynamic_array.svg",
      "item_group": "Data Structure"
    },

    {
      "item_id": 23,
      "item_name": "Tuple",
      "item_summary": "A data structure with ordered list of elements of different types",
      "item_description": "A tuple is a data structure which is a list of ordered elements. A tuple is usually represented as a comma-seperated list of the elements and enclosed in parentheses. Tuple elements usually can't be modified or removed after they are created which makes them immutable. Tuples mostly perform batter than a standard array and they are good choice for storing a list of items which have static value, different types or length.",
      "item_image": "img_tuple",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Tuple",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Data Structure"
    },

    {
      "item_id": 24,
      "item_name": "Set",
      "item_summary": "A data structure to store unique values ",
      "item_description": "A set is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.\n\nSome set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements — such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets, allow also the insertion and deletion of elements from the set.",
      "item_image": "img_set",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Set_(abstract_data_type)",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Data Structure"
    },

    {
      "item_id": 25,
      "item_name": "Singleton",
      "item_summary": "A design pattern that restricts the instantiation of a class to one single instance",
      "item_description": "The singleton pattern is a software design pattern that restricts the instantiation of a class to one \"single\" instance. This is useful when exactly one object is needed to coordinate actions across the system.\n\nThe singleton design pattern is one of the twenty-three well-known \"Gang of Four\" design patterns that describe how to solve recurring design problems to design flexible and reusable object-oriented software, that is, objects that are easier to implement, change, test, and reuse. \n\nCritics consider the singleton to be an anti-pattern in that it is frequently used in scenarios where it is not beneficial, introduces unnecessary restrictions in situations where a sole instance of a class is not actually required, and introduces global state into an application.",
      "item_image": "img_singleton",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Singleton_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Singleton_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 26,
      "item_name": "Factory",
      "item_summary": "A design pattern for creating objects",
      "item_description": "The factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created. This is done by creating objects by calling a factory method—either specified in an interface and implemented by child classes, or implemented in a base class and optionally overridden by derived classes—rather than by calling a constructor.\n\nThe Factory Method [1] design pattern is one of the \"Gang of Four\" design patterns that describe how to solve recurring design problems to design flexible and reusable object-oriented software, that is, objects that are easier to implement, change, test, and reuse.\n\nThe Factory Method design pattern is used instead of the regular class constructor, decoupling the construction of objects from the objects themselves. Creating an object directly within the class that requires or uses the object is inflexible because it commits the class to a particular object and makes it impossible to change the instantiation independently of the class. A change to the instantiator would require a change to the class code which we would rather not touch. This is referred to as code coupling and the Factory method pattern assists in decoupling the code. The Factory Method design pattern is used by first defining a separate operation, a factory method, for creating an object, and then using this factory method by calling it to create the object. This enables writing of subclasses that decide how a parent object is created and what type of objects the parent contains.",
      "item_image": "img_factory",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Factory_method_pattern",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Factory_Method_UML_class_diagram.png",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 27,
      "item_name": "Abstract Factory",
      "item_summary": "A deisgn pattern works as a super-factory which creates other factories",
      "item_description": "The abstract factory pattern provides a way to encapsulate a group of individual factories that have a common theme without specifying their concrete classes. This pattern separates the details of implementation of a set of objects from their general usage and relies on object composition, as object creation is implemented in methods exposed in the factory interface. The Abstract Factory design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nA factory is the location of a concrete class in the code at which objects are constructed. The intent in employing the pattern is to insulate the creation of objects from their usage and to create families of related objects without having to depend on their concrete classes. This allows for new derived types to be introduced with no change to the code that uses the base class.\n\n\nUse of this pattern makes it possible to interchange concrete implementations without changing the code that uses them, even at runtime. However, employment of this pattern, as with similar design patterns, may result in unnecessary complexity and extra work in the initial writing of code. Additionally, higher levels of separation and abstraction can result in systems that are more difficult to debug and maintain.\n\nAbstract Factory design pattern solves problems such as h ow can an application be independent of how its objects are created. How can a class be independent of how the objects it requires are created. How can families of related or dependent objects be created.",
      "item_image": "img_abstract_factory",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Abstract_factory_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Abstract_factory_UML.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 28,
      "item_name": "Decorator",
      "item_summary": "A design pattern used to modify the functionality of an object at runtime",
      "item_description": "In object-oriented programming, the decorator pattern is a design pattern that allows behavior to be added to an individual object, dynamically, without affecting the behavior of other objects from the same class. The decorator pattern is structurally nearly identical to the chain of responsibility pattern, the difference being that in a chain of responsibility, exactly one of the classes handles the request, while for the decorator, all classes handle the request.\n\nThe decorator design pattern is one of the twenty-three well-known GoF design patterns; these describe how to solve recurring design problems and design flexible and reusable object-oriented software—that is, objects which are easier to implement, change, test, and reuse.\n\nBy using decorator design pattern, Responsibilities are added to (and removed from) an object dynamically at run-time and a flexible alternative to subclassing for extending functionality are provided.\n\nThe decorator pattern can be used to extend (decorate) the functionality of a certain object statically, or in some cases at run-time, independently of other instances of the same class, provided some groundwork is done at design time. This is achieved by designing a new Decorator class that wraps the original class. This pattern is designed so that multiple decorators can be stacked on top of each other, each time adding a new functionality to the overridden methods.",
      "item_image": "img_decorator",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Decorator_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Decorator_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 29,
      "item_name": "Adapter",
      "item_summary": "A deisgn pattern allowing two incompatible interfaces to work together\n",
      "item_description": "The adapter pattern is a software design pattern that allows the interface of an existing class to be used as another interface. It is often used to make existing classes work with others without modifying their source code.\n\nThe adapter design pattern is one of the twenty-three well-known Gang of Four design patterns. The adapter design pattern solves problems such as How can a class be reused that does not have an interface that a client requires? How can classes that have incompatible interfaces work together? How can an alternative interface be provided for a class?\n\nThe key idea in this pattern is to work through a separate adapter that adapts the interface of an (already existing) class without changing it.\n\nAn adapter allows two incompatible interfaces to work together. This is the real-world definition for an adapter. Interfaces may be incompatible, but the inner functionality should suit the need. The adapter design pattern allows otherwise incompatible classes to work together by converting the interface of one class into an interface expected by the clients.",
      "item_image": "img_adapter",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Adapter_pattern",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Adapter_using_delegation_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 30,
      "item_name": "Observer",
      "item_summary": " A design pattern which notifies other objects when one object changes state",
      "item_description": "The observer is a software design pattern in which an object, called the subject, maintains a list of its dependents, called observers, and notifies them automatically of any state changes, usually by calling one of their methods. It is one of the twenty-three well-known \"Gang of Four\" design patterns.\n\nIt is mainly used to implement distributed event handling systems, in \"event driven\" software. In those systems, the subject is usually called a \"stream of events\" or \"stream source of events\", while the observers are called \"sink of events\". This pattern then perfectly suits any process where data arrives through I/O, that is, where data is not available to the CPU at startup, but can arrive \"randomly\" (HTTP requests, user input from keyboard/mouse/ etc). Most modern languages have built-in \"event\" constructs which implement the observer pattern components.\n\nThe Observer pattern addresses the following problems: A one-to-many dependency between objects should be defined without making the objects tightly coupled. It should be ensured that when one object changes state an open-ended number of dependent objects are updated automatically. It should be possible that one object can notify an open-ended number of other objects.",
      "item_image": "img_observer",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Observer_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Observer_w_update.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 31,
      "item_name": "Strategy",
      "item_summary": "A design pattern that enables selecting an algorithm at runtime",
      "item_description": "The strategy pattern (also known as the policy pattern) is a behavioral software design pattern that enables selecting an algorithm at runtime. Instead of implementing a single algorithm directly, code receives run-time instructions as to which in a family of algorithms to use. \n\nStrategy lets the algorithm vary independently from clients that use it. Strategy is one of the patterns included in the influential book Design Patterns. Deferring the decision about which algorithm to use until runtime allows the calling code to be more flexible and reusable.\n\nAccording to the strategy pattern, the behaviors of a class should not be inherited. Instead they should be encapsulated using interfaces. The strategy pattern uses composition instead of inheritance. In the strategy pattern, behaviors are defined as separate interfaces and specific classes that implement these interfaces. This allows better decoupling between the behavior and the class that uses the behavior. The behavior can be changed without breaking the classes that use it, and the classes can switch between behaviors by changing the specific implementation used without requiring any significant code changes.",
      "item_image": "img_strategy",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Strategy_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:StrategyPattern_IBrakeBehavior.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 32,
      "item_name": "Composite",
      "item_summary": " A design pattern which composes objects into tree structures ",
      "item_description": "The composite pattern is a partitioning design pattern. The composite pattern describes a group of objects that are treated the same way as a single instance of the same type of object. The intent of a composite is to \"compose\" objects into tree structures to represent part-whole hierarchies. Implementing the composite pattern lets clients treat individual objects and compositions uniformly. The Composite design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nWhen dealing with Tree-structured data, programmers often have to discriminate between a leaf-node and a branch. This makes code more complex, and therefore, more error prone. The solution is an interface that allows treating complex and primitive objects uniformly. In object-oriented programming, a composite is an object designed as a composition of one-or-more similar objects, all exhibiting similar functionality. This is known as a \"has-a\" relationship between objects.[4] The key concept is that you can manipulate a single instance of the object just as you would manipulate a group of them.\n\nComposite should be used when clients ignore the difference between compositions of objects and individual objects. If programmers find that they are using multiple objects in the same way, and often have nearly identical code to handle each of them, then composite is a good choice, it is less complex in this situation to treat primitives and composites as homogeneous.",
      "item_image": "img_composite",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Composite_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Composite_UML_class_diagram_(fixed).svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 33,
      "item_name": "Delegation",
      "item_summary": "A design patternt to make composition as powerful for reuse as inheritance",
      "item_description": "The delegation pattern is an object-oriented design pattern that allows object composition to achieve the same code reuse as inheritance. In delegation, an object handles a request by delegating to a second object (the delegate). The delegate is a helper object, but with the original context. \n\nDelegation is a way to make composition as powerful for reuse as inheritance [Lie86, JZ91]. In delegation, two objects are involved in handling a request: a receiving object delegates operations to its delegate. This is analogous to subclasses deferring requests to parent classes.",
      "item_image": "img_delegation",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Delegation_pattern",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 34,
      "item_name": "Bridge",
      "item_summary": "A design pattern splits a class into abstraction and implementation hierarchies",
      "item_description": "The bridge pattern is meant to \"decouple an abstraction from its implementation so that the two can vary independently\", introduced by the Gang of Four. The bridge uses encapsulation, aggregation, and can use inheritance to separate responsibilities into different classes.\n\nWhen a class varies often, the features of object-oriented programming become very useful because changes to a program's code can be made easily with minimal prior knowledge about the program. The bridge pattern is useful when both the class and what it does vary often. The class itself can be thought of as the abstraction and what the class can do as the implementation. The bridge pattern can also be thought of as two layers of abstraction.\n\nThe bridge pattern is often confused with the adapter pattern, and is often implemented using the object adapter pattern. The Bridge design pattern is one of the twenty-three well-known GoF design patterns.\n\nBridge design pattern can solve the problems such as an abstraction and its implementation should be defined and extended independently from each other or a compile-time binding between an abstraction and its implementation should be avoided so that an implementation can be selected at run-time.",
      "item_image": "img_bridge",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"en.wikipedia.org/wiki/Bridge_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Bridge_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 35,
      "item_name": "Builder",
      "item_summary": "A design pattern for building a complex object by using simple objects",
      "item_description": "The builder pattern is a design pattern designed to provide a flexible solution to various object creation problems in object-oriented programming. The intent of the Builder design pattern is to separate the construction of a complex object from its representation. It is one of the Gang of Four design patterns.\n\nThe Builder design pattern solves problems like: How can a class (the same construction process) create different representations of a complex object? How can a class that includes creating a complex object be simplified? Creating and assembling the parts of a complex object directly within a class is inflexible. It commits the class to creating a particular representation of the complex object and makes it impossible to change the representation later independently from (without having to change) the class.\n\nThe intent of the Builder design pattern is to separate the construction of a complex object from its representation. By doing so the same construction process can create different representations.\n\nAdvantages of the Builder pattern include: allows you to vary a product's internal representation, encapsulates code for construction and representation, provides control over steps of construction process.",
      "item_image": "img_builder",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Builder_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Builder_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 36,
      "item_name": "Visitor",
      "item_summary": "A design pattern which defines a new operation without changing the classes",
      "item_description": "The visitor design pattern is a way of separating an algorithm from an object structure on which it operates. A practical result of this separation is the ability to add new operations to existing object structures without modifying the structures. \n\nIn essence, the visitor allows adding new virtual functions to a family of classes, without modifying the classes. The Visitor design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nWhen new operations are needed frequently and the object structure consists of many unrelated classes, it's inflexible to add new subclasses each time a new operation is required because distributing all these operations across the various node classes leads to a system that's hard to understand, maintain, and change. Visitor pattern offers solutions by defining a separate (visitor) object that implements an operation to be performed on elements of an object structure. Clients traverse the object structure and call a dispatching operation accept(visitor) on an element — that \"dispatches\" (delegates) the request to the \"accepted visitor object\". The visitor object then performs the operation on the element (\"visits the element\"). This makes it possible to create new operations independently from the classes of an object structure by adding new visitor objects.\n\nThe nature of the Visitor makes it an ideal pattern to plug into public APIs thus allowing its clients to perform operations on a class using a \"visiting\" class without having to modify the source.",
      "item_image": "img_visitor",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Visitor_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Visitor_design_pattern.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 37,
      "item_name": "State",
      "item_summary": " A design pattern which allows an object to alter its behavior when its internal state changes",
      "item_description": "The state pattern is a behavioral software design pattern that allows an object to alter its behavior when its internal state changes. This pattern is close to the concept of finite-state machines. The state pattern can be interpreted as a strategy pattern, which is able to switch a strategy through invocations of methods defined in the pattern's interface.\n\nThe state pattern is used in computer programming to encapsulate varying behavior for the same object, based on its internal state. This can be a cleaner way for an object to change its behavior at runtime without resorting to conditional statements and thus improve maintainability. The state design pattern is one of twenty-three design patterns documented by the Gang of Four.\n\nThe state pattern is set to solve two main problems. An object should change its behavior when its internal state changes. State-specific behavior should be defined independently. That is, adding new states should not affect the behavior of existing states.",
      "item_image": "img_state",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/State_pattern",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:State_Design_Pattern_UML_Class_Diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 38,
      "item_name": "Template Method",
      "item_summary": "A design pattern which allows to define the skeleton of an algorithm",
      "item_description": "The template method is one of the behavioral design patterns identified in the book Design Patterns. The template method is a method in a superclass, usually an abstract superclass, and defines the skeleton of an operation in terms of a number of high-level steps. These steps are themselves implemented by additional helper methods in the same class as the template method.\n\nThe helper methods may be either abstract methods, for which case subclasses are required to provide concrete implementations, or hook methods, which have empty bodies in the superclass. Subclasses can (but are not required to) customize the operation by overriding the hook methods. The intent of the template method is to define the overall structure of the operation, while allowing subclasses to refine, or redefine, certain steps.\n\nThis pattern has two main parts. The \"template method\" is implemented as a method in a base class (usually an abstract class).  In the template method, portions of the algorithm that may vary are implemented by sending self messages that request the execution of additional helper methods. In the base class, these helper methods are given a default implementation, or none at all (that is, they may be abstract methods). Subclasses of the base class \"fill in\" the empty or \"variant\" parts of the \"template\" with specific algorithms that vary from one subclass to another. It is important that subclasses do not override the template method itself.\n\nAt run-time, the algorithm represented by the template method is executed by sending the template message to an instance of one of the concrete subclasses. Through inheritance, the template method in the base class starts to execute. When the template method sends a message to self requesting one of the helper methods, the message will be received by the concrete sub-instance. If the helper method has been overridden, the overriding implementation in the sub-instance will execute; if it has not been overridden, the inherited implementation in the base class will execute. This mechanism ensures that the overall algorithm follows the same steps every time, while allowing the details of some steps to depend on which instance received the original request to execute the algorithm.",
      "item_image": "img_template_method",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Template_method_pattern",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Template_Method_UML_class_diagram.svg",
      "item_group": "Design Pattern"
    },

    {
      "item_id": 39,
      "item_name": "Dynamic Programming",
      "item_summary": "A programming method for solving a complex problem by breaking it down into simpler subproblems",
      "item_description": "Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. Wherever we see a recursive solution that has repeated calls for same inputs, we can optimize it using Dynamic Programming. The idea is to simply store the results of subproblems, so that we do not have to re-compute them when needed later.  If a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\n\nThere are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems. If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called \"divide and conquer\" instead. This is why merge sort and quick sort are not classified as dynamic programming problems. \n\nOptimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of recursion. Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems.  Dynamic programming takes account of this fact and solves each sub-problem only once.",
      "item_image": "img_dynamic_programming",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Dynamic_programming#Computer_programming",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Shortest_path_optimal_substructure.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 40,
      "item_name": "Greedy Algorithm",
      "item_summary": "An algorithm paradigm which builds a solution by making optimum choice at each stage",
      "item_description": "A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum. It builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.\n\nGreedy algorithms mostly (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. They can make commitments to certain choices too early which prevent them from finding the best overall solution later. Nevertheless, they are useful because they are quick to think up and often give good approximations to the optimum.\n\nIf a greedy algorithm can be proven to yield the global optimum for a given problem class, it typically becomes the method of choice because it is faster than other optimization methods like dynamic programming. Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees, and the algorithm for finding optimum Huffman trees.",
      "item_image": "img_greedy_algorithm",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Greedy_algorithm",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Greedy_algorithm_36_cents.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 41,
      "item_name": "Divide And Conquer Algorithm",
      "item_summary": "An algorithm paradigm which breaks recursively a problem down into two or more sub-problems",
      "item_description": "Divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\n\nThis divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort). The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. \n\nDivide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem.",
      "item_image": "img_merge_sort",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:Merge_sort_algorithm_diagram.svg ",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 42,
      "item_name": "Recursion",
      "item_summary": "The process in which a function calls itself ",
      "item_description": "Recursion is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem. Such problems can generally be solved by iteration, but this needs to identify and index the smaller instances at programming time. At the opposite, recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.\n\nA common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.\n\nA recursive function definition has one or more base cases, meaning inputs for which the function produces a result trivially (without recurring), and one or more recursive cases, meaning inputs for which the program recurs (calls itself). The job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached.",
      "item_image": "img_recursion",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Recursion_(computer_science)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Recursive1.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 43,
      "item_name": "Analysis of Algorithms",
      "item_summary": "The process of finding the computational complexity of algorithms",
      "item_description": "The analysis of algorithms is the process of finding the computational complexity of algorithms – the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). \n\nAn algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same length may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest. When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\n\nIn theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. For instance, binary search is said to run in a number of steps proportional to the logarithm of the length of the sorted list being searched, or in O(log(n)), colloquially \"in logarithmic time\". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency.",
      "item_image": "img_analysis_of_algorithms",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Analysis_of_algorithms",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Comparison_computational_complexity.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 44,
      "item_name": "Regular Expressions",
      "item_summary": "The patterns used to match character combinations in strings",
      "item_description": "A regular expression (shortened as regex or regexp) is a sequence of characters that define a search pattern. Usually such patterns are used by string searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. \n\nRegular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities. Many programming languages provide regex capabilities either built-in or via libraries.\n\nA regular expression, often called a pattern, specifies a set of strings required for a particular purpose. A regex pattern matches a target string. A simple way to specify a finite set of strings is to list its elements or members.The phrase regular expressions, also called regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex a., a is a literal character which matches just 'a', while '.' is a metacharacter that matches every character except a newline. \n\nA very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both \"serialise\" and \"serialize\". Wildcards also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base.",
      "item_image": "img_regular_expessions",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Regular_expression",
      "item_image_source": "Wikimedia",
      "item_image_source_url":"https://commons.wikimedia.org/wiki/File:OOjs_UI_icon_regular-expression.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 45,
      "item_name": "Inheritance",
      "item_summary": "The mechanism of a class to derive properties and behaviour from another class",
      "item_description": "In object-oriented programming (OOP), inheritance is the mechanism of basing an object or class upon another object or class, retaining similar implementation. Also defined as deriving new classes from existing ones such as super class or base class and then forming them into a hierarchy of classes. In most class-based object-oriented languages, an object created through inheritance, a \"child object\", acquires all the properties and behaviors of the parent object. Inheritance was invented in 1969 for Simula programming language and is now used throughout many object-oriented programming languages. Inheritance is one of the core concepts of OOP.\n\nInheritance allows programmers to create classes that are built upon existing classes, to specify a new implementation while maintaining the same behaviors (realizing an interface), to reuse code and to independently extend original software via public classes and interfaces. The relationships of objects or classes through inheritance give rise to a directed graph. \n\nInheritance is contrasted with object composition, where one object contains another object (or objects of one class contain objects of another class). Composition implements a has-a relationship, whereas inheritance only reuses implementation and establishes a syntactic relationship, not necessarily a semantic relationship.\n\nMany object-oriented programming languages permit a class or object to replace the implementation of an aspect—typically a behavior—that it has inherited. This process is called overriding.",
      "item_image": "img_inheritance",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)",
      "item_image_source": "Wikipedia",
      "item_image_source_url":"https://en.wikipedia.org/wiki/File:Method_overriding_in_subclass.svg",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 46,
      "item_name": "Polymorphism",
      "item_summary": "The concept of objects of different types can be accessed through the same interface",
      "item_description": "Polymorphism describes the concept that objects of different types can be accessed through the same interface. Each type can provide its own, independent implementation of this interface. It is one of the core concepts of object-oriented programming (OOP).\n\nThe most common use of polymorphism in OOP occurs when a parent class reference is used to refer to a child class object. Polymorphism can be distinguished by when the implementation is selected: statically at compile time or dynamically at run time. The corresponding forms of polymorphism are accordingly called static polymorphism and dynamic polymorphism. Static polymorphism executes faster, because there is no dynamic dispatch overhead, but requires additional compiler support. Dynamic polymorphism is more flexible but slower—for example, dynamic polymorphism allows duck typing, and a dynamically linked library may operate on objects without knowing their full type.\n\nSome languages employ the idea of subtyping (also called subtype polymorphism or inclusion polymorphism) to restrict the range of types that can be used in a particular case of polymorphism. Subtype polymorphism is usually resolved dynamically.",
      "item_image": "img_polymorphism",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Polymorphism_(computer_science)",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 47,
      "item_name": "Encapsulation",
      "item_summary": "The concept of restricting direct access to some of an object's components",
      "item_description": "In object-oriented programming (OOP), encapsulation refers to the restricting of direct access to some of an object's components. Encapsulation is used to hide the values or state of a structured data object inside a class, preventing unauthorized parties' direct access to them. Publicly accessible methods are generally provided in the class (so-called \"getters\" and \"setters\") to access the values, and other client classes call these methods to retrieve and modify the values within the object. Encapsulation is one of the core concepts of OOP.\n\nUnder the definition that encapsulation \"can be used to hide data members and member functions\", the internal representation of an object is generally hidden from view outside of the object's definition. Typically, only the object's own methods can directly inspect or manipulate its fields. Hiding the internals of the object protects its integrity by preventing users from setting the internal data of the component into an invalid or inconsistent state. A supposed benefit of encapsulation is that it can reduce system complexity, and thus increase robustness, by allowing the developer to limit the interdependencies between software components.",
      "item_image": "img_encapsulation",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 48,
      "item_name": "Abstraction",
      "item_summary": "The concept of handling complexity by hiding unnecessary details",
      "item_description": "Abstraction, in general, is a fundamental concept in computer science and software development. The process of abstraction can also be referred to as modeling. Models can also be considered types of abstractions per their generalization of aspects of reality. In simple terms, abstraction is removing irrelevant data so a program is easier to understand. Abstraction is one of the core concepts of Object oriented programming.\n\nThe software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions.\n\nData abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the interface to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.",
      "item_image": "img_abstraction",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Abstraction_(computer_science)",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 49,
      "item_name": "Unit Testing",
      "item_summary": "Software testing where individual units or components of a software are tested",
      "item_description": "Unit testing is a software testing method by which individual units of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures, are tested to determine whether they are fit for use.\n\nUnit tests are typically automated tests written and run by software developers to ensure that a section of an application (known as the \"unit\") meets its design and behaves as intended.  In object-oriented programming, a unit is often an entire interface, such as a class, but could be an individual method.[3] By writing tests first for the smallest testable units, then the compound behaviors between those, one can build up comprehensive tests for complex applications. To isolate issues that may arise, each test case should be tested independently. Substitutes such as method stubs, mock objects, fakes, and test harnesses can be used to assist testing a module in isolation.\n\nThe goal of unit testing is to isolate each part of the program and show that the individual parts are correct.A unit test provides a strict, written contract that the piece of code must satisfy. Unit testing finds problems early in the development cycle. This includes both bugs in the programmer's implementation and flaws or missing parts of the specification for the unit.",
      "item_image": "img_unit_testing",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Unit_testing",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Programming Concept"
    },

    {
      "item_id": 50,
      "item_name": "Access Modifiers",
      "item_summary": "Keywords which set the accessibility of classes, methods, and other members\n",
      "item_description": "Access modifiers (or access specifiers) are keywords in object-oriented languages that set the accessibility of classes, methods, and other members. Access modifiers are a specific part of programming language syntax used to facilitate the encapsulation of components.\n\nWhen the class is declared as public, it is accessible to other classes defined in the same package as well as those defined in other packages. This is the most commonly used specifier for classes. A class cannot be declared as private. If no access specifier is stated, the default access restrictions will be applied. The class will be accessible to other classes in the same package but will be inaccessible to classes outside the package. When we say that a class is inaccessible, it simply means that we cannot create an object of that class or declare a variable of that class type. The protected access specifier too cannot be applied to a class.\n\nC++ uses the three modifiers called public, protected, and private.[4] C# has the modifiers public, protected ,internal, private, protected internal, and private protected. Java has public, package, protected, and private. The access modifier package is the default and used, if any other access modifier keyword is missing. The meaning of these modifiers may differ from one language to another.",
      "item_image": "img_access_modifiers",
      "item_favourite": false,
      "item_text_source": "Wikipedia",
      "item_text_source_url":"https://en.wikipedia.org/wiki/Access_modifiers",
      "item_image_source": "none",
      "item_image_source_url":"none",
      "item_group": "Programming Concept"
    }
  ]
}