{
  "drinks": [

    {
      "item_name": "Binary Search Algorithm",
      "item_summary": "A search algorithm to find the position of a target value within a sorted array",
      "item_description": "Binary search, (also known as half-interval search, logarithmic search, or binary chop) is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.\n\nBinary search runs in logarithmic time in the worst case, making {\\displaystyle O(\\log n)}O(\\log n) comparisons, where {\\displaystyle n}n is the number of elements in the array, the {\\displaystyle O}O is Big O notation, and {\\displaystyle \\log }\\log  is the logarithm.[6] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.\n\nBinary search works on sorted arrays. Binary search begins by comparing an element in the middle of the array with the target value. If the target value matches the element, its position in the array is returned. If the target value is less than the element, the search continues in the lower half of the array. If the target value is greater than the element, the search continues in the upper half of the array. By doing this, the algorithm eliminates the half in which the target value cannot lie in each iteration.\n\nIn 1946, John Mauchly made the first mention of binary search as part of the Moore School Lectures, a seminal and foundational college course in computing. In 1957, William Wesley Peterson published the first method for interpolation search. Every published binary search algorithm worked only for arrays whose length is one less than a power of two until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays.In 1962, Hermann Bottenbruch presented an ALGOL 60 implementation of binary search that placed the comparison for equality at the end, increasing the average number of iterations by one, but reducing to one the number of comparisons per iteration. The uniform binary search was developed by A. K. Chandra of Stanford University in 1971. In 1986, Bernard Chazelle and Leonidas J. Guibas introduced fractional cascading as a method to solve numerous search problems in computational geometry.\n\nFor more information: Wikipedia\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Binary_search_algorithm\nImage: Wikipedia",
      "item_image": "img_binary_search",
      "item_favourite": true
    },

    {
      "item_name": "Quick Sort Algorithm",
      "item_summary": "An efficient sorting algorithm",
      "item_description":"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm. Developed by British computer scientist Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.\n\nQuicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.\n\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.\n\nMathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.\n\nQuicksort is a divide and conquer algorithm. It first divides the input array into two smaller sub-arrays: the low elements and the high elements. It then recursively sorts the sub-arrays. \n\nFor more information: Wikipedia\n\nSource:\nImage: Wikimedia\n\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Quicksort",
      "item_image": "img_quick_sort",
      "item_favourite": false
    },

    {
      "item_name": "Insertion Sort Algorithm",
      "item_summary": "A sorting algorithm that builds the final sorted array one item at a time",
      "item_description": "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages with having simple implementation, being efficient for small data sets and being stable. When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\n\nInsertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\n\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\n\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. \n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Insertion_sort\n\nImage: Wikimedia",
      "item_image": "img_insertion_sort",
      "item_favourite": false
    },

    {
      "item_name": "Selection Sort Algorithm",
      "item_summary": "An in-place comparison sorting algorithm. ",
      "item_description": "Selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations.\n\nThe algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.\n\nThe time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort. One thing which distinguishes selection sort from other sorting algorithms is that it makes the minimum possible number of swaps, n − 1 in the worst case.\n\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of Θ(n2)), selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the kth iteration, the first k elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the k + 1st element, while selection sort must scan all remaining elements to find the k + 1st element.\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Selection_sort\nImage: Wikimedia\nhttps://upload.wikimedia.org/wikipedia/commons/6/67/SelectionSort.jpg",
      "item_image": "img_selection_sort",
      "item_favourite": false
    },

    {
      "item_name": "Bubble Sort Algorithm",
      "item_summary": "A simple sorting algorithm that repeatedly steps through the list",
      "item_description": "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements \"bubble\" to the top of the list.\n\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as merge sort are used by the sorting libraries built into popular programming languages.\n\nBubble sort has a worst-case and average complexity of О(n2), where n is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often O(n log n). Even other О(n2) sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. Therefore, bubble sort is not a practical sorting algorithm.\n\nThe only significant advantage that bubble sort has over most other algorithms, even quicksort, but not insertion sort, is that the ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only O(n). By contrast, most other algorithms, even those with better average-case complexity, perform their entire sorting process on the set and thus are more complex. However, not only does insertion sort share this advantage, but it also performs better on a list that is substantially sorted (having a small number of inversions).\n\nBubble sort should be avoided in the case of large collections. It will not be efficient in the case of a reverse-ordered collection.\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Bubble_sort\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:BubbleSort.jpg",
      "item_image": "img_bubble_sort",
      "item_favourite": false
    },

    {
      "item_name": "Merge Sort Algorithm",
      "item_summary": "An efficient general-purpose comparison-based sorting algorithm",
      "item_description": "Merge sort is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945.\n\nA merge sort works by dividing the unsorted list into n sublists, each containing one element (a list of one element is considered sorted) then repeatedly merging sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list. In sorting n objects, merge sort has an average and worst-case performance of O(n log n). \n\nMerge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort. Merge sort's most common implementation does not sort in place. Therefore, the memory size of the input must be allocated for the sorted output to be stored in.\n\nAlthough heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(n). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\n\nSources:\nText: Wikipedia \nhttps://en.wikipedia.org/wiki/Merge_sort\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Merge_sort_algorithm_diagram.svg ",
      "item_image": "img_merge_sort",
      "item_favourite": false
    },

    {
      "item_name": "Breadth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level. It uses the opposite strategy as depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes.\n\nBFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, but this was not published until 1972. It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze and later developed by C. Y. Lee into a wire routing algorithm which was published in 1961.\n\nBFS uses a queue and it checks whether a vertex has been discovered before enqueueing the vertex rather than delaying this check until the vertex is dequeued from the queue. The queue contains the frontier along which the algorithm is currently searching. Nodes can be labelled as discovered by storing them in a set, or by an attribute on each node, depending on the implementation. Note that the word node is usually interchangeable with the word vertex. The parent attribute of each node is useful for accessing the nodes in a shortest path, for example by backtracking from the destination node up to the starting node, once the BFS has been run, and the predecessors nodes have been set. Breadth-first search produces a so-called breadth first tree.\n\nBreadth-first search can be used to solve many problems in graph theory such as finding the shortest path between two nodes with path length measured by number of edges.\n\nSources & more info:\n\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Breadth-first_search\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Breadth-first_tree.svg",
      "item_image": "img_breadth_first_search",
      "item_favourite": false
    },

    {
      "item_name": "Depth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.\n\nFor applications of DFS in relation to specific domains, such as searching for solutions in artificial intelligence or web-crawling, the graph to be traversed is often either too large to visit in its entirety or infinite. DFS may suffer from non-termination. In such cases, search is only performed to a limited depth; due to limited resources, such as memory or disk space, one typically does not use data structures to keep track of the set of all previously visited vertices. When an appropriate depth limit is not known a priori, iterative deepening depth-first search applies DFS repeatedly with a sequence of increasing limits. \n\nIn the artificial intelligence mode of analysis, with a branching factor greater than one, iterative deepening increases the running time by only a constant factor over the case in which the correct depth limit is known due to the geometric growth of the number of nodes per level.\n\nSources & more info:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Depth-first_search\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Depth-first-tree.svg",
      "item_image": "img_depth_first_search",
      "item_favourite": false
    },

    {
      "item_name": "Heap Sort Algorithm",
      "item_summary": "A comparison-based sorting algorithm",
      "item_description": "Heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step. Heapsort was invented by J. W. J. Williams in 1964.[2] This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort. \n\nThe heapsort algorithm can be divided into two parts. In the first step, a heap is built out of the data (see Binary heap § Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nThe Heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm. Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O(n2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. Thus, because of the O(n log n) upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort. Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort such as being a stable sort.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Heapsort\n\nImage: Wikimedia \nhttps://commons.wikimedia.org/wiki/File:Heap_sort_algorithm-phase1.svg",
      "item_image": "img_heap_sort",
      "item_favourite": false
    },

    {
      "item_name": "Kruskal's Algorithm",
      "item_summary": "A greedy algorithm to find a minimum spanning tree for a connected weighted graph.",
      "item_description": "Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. A Minimum Spanning Tree is a spanning tree of a connected, undirected graph. It connects all vertices together with the minimal weighting for its edges.\n\nThis is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component). This algorithm was written by Joseph Kruskal and first appeared in in 1956.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Kruskal's_algorithm\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Kruskal_Algorithm_1.svg",
      "item_image": "img_kruskals_algorithm",
      "item_favourite": false
    },

    {
      "item_name": "Dijkstra's Algorithm",
      "item_summary": "An algorithm for finding the shortest paths between nodes in a graph",
      "item_description": "Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\n\nThe algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.\n\nFor a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithm is network routing protocols.\n\nDijkstra's algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.\n\nSource & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Dijkstra_algorithm_example_27.svg",
      "item_image": "img_dijkstras",
      "item_favourite": false
    },

    {
      "item_name": "Stack",
      "item_summary": "A collection of elements wşth push and pop operations.",
      "item_description": "A stack is an abstract data type that serves as a collection of elements, with two principal operations. These operataions are push, which adds an element to the collection, and pop, which removes the most recently added element that was not yet removed.\n\nThe order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack.[1] The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.\n\nConsidered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search.\n\nKlaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea of a stack in 1955 and filed a patent in 1957.\n\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Stack_(abstract_data_type)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Lifo_stack.png",
      "item_image": "img_stack",
      "item_favourite": false
    },

    {
      "item_name": "Queue",
      "item_summary": "A linear collection of objects that are inserted and removed according to the FIFO principle. ",
      "item_description": "A queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and removal from the other end of the sequence. By convention, the end of the sequence at which elements are added is called the back, tail, or rear of the queue and the end at which elements are removed is called the head or front of the queue, analogously to the words used when people line up to wait for goods or services.\n\nThe operation of adding an element to the rear of the queue is known as enqueue, and the operation of removing an element from the front is known as dequeue. Other operations may also be allowed, often including a peek or front operation that returns the value of the next element to be dequeued without dequeuing it.\n\nThe operations of a queue make it a first-in-first-out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. A queue is an example of a linear data structure, or more abstractly a sequential collection. Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Queue_(abstract_data_type)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Data_Queue.svg",
      "item_image": "img_queue",
      "item_favourite": false
    },

    {
      "item_name": "Graph",
      "item_summary": "A non-linear data structure consisting of nodes and edges",
      "item_description": "A graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from the field of graph theory within mathematics.\n\nA graph data structure consists of a finite (and possibly mutable) set of vertices (also called nodes or points), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as edges (also called links or lines), and for a directed graph are also known as arrows. The vertices may be part of the graph structure, or may be external entities represented by integer indices or references. A graph data structure may also associate to each edge some edge value, such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Graph_(abstract_data_type)\n\nImage: Wikipedi\nhttps://en.wikipedia.org/wiki/File:Directed.svg",
      "item_image": "img_graph",
      "item_favourite": false
    },

    {
      "item_name": "Heap",
      "item_summary": "A specialized tree-based data structure",
      "item_description": "A heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.[2] The node at the \"top\" of the heap (with no parents) is called the root node.\n\nThe heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as \"heaps\", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority.\n\nA common implementation of a heap is the binary heap, in which the tree is a binary tree. The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Heap_(data_structure)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Max-Heap.svg",
      "item_image": "img_heap",
      "item_favourite": false
    },

    {
      "item_name": "Linked List",
      "item_summary": "A linear data structure consists of nodes where each node contains a data field and a link to the next node",
      "item_description": "A linked list is a linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence. In its most basic form, each node contains: data, and a reference (in other words, a link) to the next node in the sequence. This structure allows for efficient insertion or removal of elements from any position in the sequence during iteration. A drawback of linked lists is that access time is linear. Faster access, such as random access, is not feasible. Arrays have better cache locality compared to linked lists. Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation.\n\nLinked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis.\n\nThe principal benefit of a linked list over a conventional array is that the list elements can be easily inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while restructuring an array at run-time is a much more expensive operation. Linked lists allow insertion and removal of nodes at any point in the list, and allow doing so with a constant number of operations by keeping the link previous to the link being added or removed in memory during list traversal.\n\nOn the other hand, since simple linked lists by themselves do not allow random access to the data or any form of efficient indexing, many basic operations—such as obtaining the last node of the list, finding a node that contains a given datum, or locating the place where a new node should be inserted—may require iterating through most or all of the list elements. Linked list are dynamic, so the length of list can increase or decrease as necessary. \n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Linked_list\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Singly-linked-list.svg",
      "item_image": "img_linked_list",
      "item_favourite": false
    },

    {
      "item_name": "Hash Table",
      "item_summary": "A data structure which stores data in an array format with unique index value",
      "item_description": "A hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found.\n\nIn many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.\n\nThe idea of hashing is to distribute the entries (key/value pairs) across an array of buckets. Given a key, the algorithm computes an index that suggests where the entry can be found.\n\nA basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests.\n\nThe main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted in advance. Although operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Hash_table\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Hash_table_3_1_1_0_1_0_0_SP.svg",
      "item_image": "img_hash_table",
      "item_favourite": false
    },

    {
      "item_name": "Tree",
      "item_summary": "A nonlinear data structure",
      "item_description": "A tree is a widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.\n\nA tree data structure can be defined recursively as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the \"children\"), with the constraints that no reference is duplicated, and none points to the root. Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node.\n\nA tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Tree_(data_structure)\n\nImage: Wikipedia",
      "item_image": "img_tree",
      "item_favourite": false
    },

    {
      "item_name": "Associative Array",
      "item_summary": "An abstract data type composed of a collection of key, value pairs",
      "item_description": "An associative array (also called map, symbol table, or dictionary) is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection. Operations associated with this data type allow the addition of a pair to the collection, the removal of a pair from the collection, the modification of an existing pair, the lookup of a value associated with a particular key.\n\nAssociative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern. In an associative array, the association between a key and a value is often known as a \"mapping\", and the same word mapping may also be used to refer to the process of creating a new association.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Associative_array\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/Associative_array",
      "item_image": "img_associative_array",
      "item_favourite": false
    },

    {
      "item_name": "Array",
      "item_summary": "a data structure consisting of a collection of elements",
      "item_description": "An array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index. The simplest type of data structure is a linear array, also called one-dimensional array.\n\nBecause the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word table is sometimes used as a synonym of array.\n\nArrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses.\n\nArrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation.\n\nArrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.\n\nArrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient, requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).\n\nSources & more information\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Array_data_structure\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Array_of_array_storage.svg",
      "item_image": "img_array",
      "item_favourite": false
    },

    {
      "item_name": "Vector",
      "item_summary": "An array with a dynamic size",
      "item_description": "Vector is generally a one-dimensional array, typically storing numbers. Unlike static arrays, which are always of a fixed size, vectors can be grown.This can be done either explicitly or by adding more data. In order to do this efficiently, the typical vector implementation grows by doubling its allocated space (rather than incrementing it) and often has more space allocated to it at any one time than it needs. This is because reallocating memory is usually an expensive operation. Therefore vectors use a dynamically allocated array to store their elements. \n\nThe vector data structure can be used to represent the mathematical vector used in linear algebra. Vectors are often used in computing in computer graphics and simulating physical systems.\n\nSources: Wikiuniversity\nhttps://en.wikiversity.org/wiki/Data_Structures_and_Algorithms/Arrays,_Lists_and_Vectors\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Array_of_array_storage.svg",
      "item_image": "img_array",
      "item_favourite": false
    },

    {
      "item_name": "Dynamic Array",
      "item_summary": "A variable-size list data structure that allows elements to be added or removed",
      "item_description": "A dynamic array (also called growable array, resizable array, dynamic table, mutable array or array list) is a random access, variable-size list data structure that allows elements to be added or removed. It is supplied with standard libraries in many modern mainstream programming languages. Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation.\n\nA dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end.\n\nA simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. \n\nElements can be added at the end of a dynamic array in constant time by using the reserved space, until this space is completely consumed. When all space is consumed, and an additional element is to be added, then the underlying fixed-sized array needs to be increased in size. Typically resizing is expensive because it involves allocating a new underlying array and copying each element from the original array. Elements can be removed from the end of a dynamic array in constant time, as no resizing is required. The number of elements used by the dynamic array contents is its logical size or size, while the size of the underlying array is called the dynamic array's capacity or physical size, which is the maximum possible size without relocating data.\n\nA dynamic array might be preferred if the maximum logical size is unknown, or difficult to calculate, before the array is allocated, it is considered that a maximum logical size given by a specification is likely to change and the amortized cost of resizing a dynamic array does not significantly affect performance or responsiveness.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Dynamic_array\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Dynamic_array.svg",
      "item_image": "img_dynamic_array",
      "item_favourite": false
    },

    {
      "item_name": "Tuple",
      "item_summary": "A data structure with ordered list of elements of different types",
      "item_description": "A tuple is a data structure that is an ordered list of elements. Often, a tuple is represented as a comma-delimited list of the elements, enclosed in parentheses. Tuples are usually immutable, that is, their elements cannot be modified or deleted after they are set. Tuples are usually more performant (their data may be accessed faster) than a standard array. Tuples are ideal for storing a list of heterogenous items (that differ in type or length) whose value does not change.\n\nSources:\nText: Computer Hope\nhttps://www.computerhope.com/jargon/t/tuple.htm",
      "item_image": "img_tuple",
      "item_favourite": false
    },

    {
      "item_name": "Set",
      "item_summary": "A data structure to store unique values ",
      "item_description": "A set is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.\n\nSome set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements — such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets, allow also the insertion and deletion of elements from the set.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Set_(abstract_data_type)",
      "item_image": "img_set",
      "item_favourite": false
    },

    {
      "item_name": "Singleton",
      "item_summary": "A design pattern that restricts the instantiation of a class to one single instance",
      "item_description": "The singleton pattern is a software design pattern that restricts the instantiation of a class to one \"single\" instance. This is useful when exactly one object is needed to coordinate actions across the system.\n\nThe singleton design pattern is one of the twenty-three well-known \"Gang of Four\" design patterns that describe how to solve recurring design problems to design flexible and reusable object-oriented software, that is, objects that are easier to implement, change, test, and reuse. \n\nCritics consider the singleton to be an anti-pattern in that it is frequently used in scenarios where it is not beneficial, introduces unnecessary restrictions in situations where a sole instance of a class is not actually required, and introduces global state into an application.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Singleton_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Singleton_UML_class_diagram.svg",
      "item_image": "img_singleton",
      "item_favourite": false
    },

    {
      "item_name": "Factory",
      "item_summary": "A design pattern for creating objects",
      "item_description": "The factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created. This is done by creating objects by calling a factory method—either specified in an interface and implemented by child classes, or implemented in a base class and optionally overridden by derived classes—rather than by calling a constructor.\n\nThe Factory Method [1] design pattern is one of the \"Gang of Four\" design patterns that describe how to solve recurring design problems to design flexible and reusable object-oriented software, that is, objects that are easier to implement, change, test, and reuse.\n\nThe Factory Method design pattern is used instead of the regular class constructor, decoupling the construction of objects from the objects themselves. Creating an object directly within the class that requires or uses the object is inflexible because it commits the class to a particular object and makes it impossible to change the instantiation independently of the class. A change to the instantiator would require a change to the class code which we would rather not touch. This is referred to as code coupling and the Factory method pattern assists in decoupling the code. The Factory Method design pattern is used by first defining a separate operation, a factory method, for creating an object, and then using this factory method by calling it to create the object. This enables writing of subclasses that decide how a parent object is created and what type of objects the parent contains.\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Factory_method_pattern\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Factory_Method_UML_class_diagram.png",
      "item_image": "img_factory",
      "item_favourite": false
    },

    {
      "item_name": "Abstract Factory",
      "item_summary": "A deisgn pattern works as a super-factory which creates other factories",
      "item_description": "The abstract factory pattern provides a way to encapsulate a group of individual factories that have a common theme without specifying their concrete classes. This pattern separates the details of implementation of a set of objects from their general usage and relies on object composition, as object creation is implemented in methods exposed in the factory interface. The Abstract Factory design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nA factory is the location of a concrete class in the code at which objects are constructed. The intent in employing the pattern is to insulate the creation of objects from their usage and to create families of related objects without having to depend on their concrete classes. This allows for new derived types to be introduced with no change to the code that uses the base class.\n\n\nUse of this pattern makes it possible to interchange concrete implementations without changing the code that uses them, even at runtime. However, employment of this pattern, as with similar design patterns, may result in unnecessary complexity and extra work in the initial writing of code. Additionally, higher levels of separation and abstraction can result in systems that are more difficult to debug and maintain.\n\nAbstract Factory design pattern solves problems such as h ow can an application be independent of how its objects are created? How can a class be independent of how the objects it requires are created? How can families of related or dependent objects be created?\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Abstract_factory_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Abstract_factory_UML.svg",
      "item_image": "img_abstract_factory",
      "item_favourite": false
    },

    {
      "item_name": "Decorator",
      "item_summary": "A design pattern used to modify the functionality of an object at runtime",
      "item_description": "In object-oriented programming, the decorator pattern is a design pattern that allows behavior to be added to an individual object, dynamically, without affecting the behavior of other objects from the same class. The decorator pattern is structurally nearly identical to the chain of responsibility pattern, the difference being that in a chain of responsibility, exactly one of the classes handles the request, while for the decorator, all classes handle the request.\n\nThe decorator design pattern is one of the twenty-three well-known GoF design patterns; these describe how to solve recurring design problems and design flexible and reusable object-oriented software—that is, objects which are easier to implement, change, test, and reuse.\n\nBy using decorator design pattern, Responsibilities are added to (and removed from) an object dynamically at run-time and a flexible alternative to subclassing for extending functionality are provided.\n\nThe decorator pattern can be used to extend (decorate) the functionality of a certain object statically, or in some cases at run-time, independently of other instances of the same class, provided some groundwork is done at design time. This is achieved by designing a new Decorator class that wraps the original class. This pattern is designed so that multiple decorators can be stacked on top of each other, each time adding a new functionality to the overridden methods.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Decorator_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Decorator_UML_class_diagram.svg",
      "item_image": "img_decorator",
      "item_favourite": false
    },

    {
      "item_name": "Adapter",
      "item_summary": "A deisgn pattern allowing two incompatible interfaces to work together\n",
      "item_description": "The adapter pattern is a software design pattern that allows the interface of an existing class to be used as another interface. It is often used to make existing classes work with others without modifying their source code.\n\nThe adapter design pattern is one of the twenty-three well-known Gang of Four design patterns. The adapter design pattern solves problems such as How can a class be reused that does not have an interface that a client requires? How can classes that have incompatible interfaces work together? How can an alternative interface be provided for a class?\n\nThe key idea in this pattern is to work through a separate adapter that adapts the interface of an (already existing) class without changing it.\n\nAn adapter allows two incompatible interfaces to work together. This is the real-world definition for an adapter. Interfaces may be incompatible, but the inner functionality should suit the need. The adapter design pattern allows otherwise incompatible classes to work together by converting the interface of one class into an interface expected by the clients.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Adapter_pattern\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Adapter_using_delegation_UML_class_diagram.svg",
      "item_image": "img_adapter",
      "item_favourite": false
    },

    {
      "item_name": "Observer",
      "item_summary": " A design pattern which notifies other objects when one object changes state",
      "item_description": "The observer is a software design pattern in which an object, called the subject, maintains a list of its dependents, called observers, and notifies them automatically of any state changes, usually by calling one of their methods. It is one of the twenty-three well-known \"Gang of Four\" design patterns.\n\nIt is mainly used to implement distributed event handling systems, in \"event driven\" software. In those systems, the subject is usually called a \"stream of events\" or \"stream source of events\", while the observers are called \"sink of events\". This pattern then perfectly suits any process where data arrives through I/O, that is, where data is not available to the CPU at startup, but can arrive \"randomly\" (HTTP requests, user input from keyboard/mouse/ etc). Most modern languages have built-in \"event\" constructs which implement the observer pattern components.\n\nThe Observer pattern addresses the following problems: A one-to-many dependency between objects should be defined without making the objects tightly coupled. It should be ensured that when one object changes state an open-ended number of dependent objects are updated automatically. It should be possible that one object can notify an open-ended number of other objects.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Observer_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Observer_w_update.svg",
      "item_image": "img_observer",
      "item_favourite": false
    },

    {
      "item_name": "Strategy",
      "item_summary": "A design pattern that enables selecting an algorithm at runtime",
      "item_description": "The strategy pattern (also known as the policy pattern) is a behavioral software design pattern that enables selecting an algorithm at runtime. Instead of implementing a single algorithm directly, code receives run-time instructions as to which in a family of algorithms to use. \n\nStrategy lets the algorithm vary independently from clients that use it. Strategy is one of the patterns included in the influential book Design Patterns. Deferring the decision about which algorithm to use until runtime allows the calling code to be more flexible and reusable.\n\nAccording to the strategy pattern, the behaviors of a class should not be inherited. Instead they should be encapsulated using interfaces. The strategy pattern uses composition instead of inheritance. In the strategy pattern, behaviors are defined as separate interfaces and specific classes that implement these interfaces. This allows better decoupling between the behavior and the class that uses the behavior. The behavior can be changed without breaking the classes that use it, and the classes can switch between behaviors by changing the specific implementation used without requiring any significant code changes.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Strategy_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:StrategyPattern_IBrakeBehavior.svg",
      "item_image": "img_strategy",
      "item_favourite": false
    },

    {
      "item_name": "Composite",
      "item_summary": " A design pattern which composes objects into tree structures ",
      "item_description": "The composite pattern is a partitioning design pattern. The composite pattern describes a group of objects that are treated the same way as a single instance of the same type of object. The intent of a composite is to \"compose\" objects into tree structures to represent part-whole hierarchies. Implementing the composite pattern lets clients treat individual objects and compositions uniformly. The Composite design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nWhen dealing with Tree-structured data, programmers often have to discriminate between a leaf-node and a branch. This makes code more complex, and therefore, more error prone. The solution is an interface that allows treating complex and primitive objects uniformly. In object-oriented programming, a composite is an object designed as a composition of one-or-more similar objects, all exhibiting similar functionality. This is known as a \"has-a\" relationship between objects.[4] The key concept is that you can manipulate a single instance of the object just as you would manipulate a group of them.\n\nComposite should be used when clients ignore the difference between compositions of objects and individual objects. If programmers find that they are using multiple objects in the same way, and often have nearly identical code to handle each of them, then composite is a good choice, it is less complex in this situation to treat primitives and composites as homogeneous. \n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Composite_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Composite_UML_class_diagram_(fixed).svg",
      "item_image": "img_composite",
      "item_favourite": false
    },

    {
      "item_name": "Delegation",
      "item_summary": "A design patternt to make composition as powerful for reuse as inheritance",
      "item_description": "The delegation pattern is an object-oriented design pattern that allows object composition to achieve the same code reuse as inheritance. In delegation, an object handles a request by delegating to a second object (the delegate). The delegate is a helper object, but with the original context. \n\nDelegation is a way to make composition as powerful for reuse as inheritance [Lie86, JZ91]. In delegation, two objects are involved in handling a request: a receiving object delegates operations to its delegate. This is analogous to subclasses deferring requests to parent classes. \n\nSources & more information:\nText:https://en.wikipedia.org/wiki/Delegation_pattern\n\nImage: www.growingwiththeweb.com\nhttps://www.growingwiththeweb.com/2012/07/design-patterns-delegation-pattern.html",
      "item_image": "img_delegation",
      "item_favourite": false
    },

    {
      "item_name": "Bridge",
      "item_summary": "A design pattern splits a class into abstraction and implementation hierarchies",
      "item_description": "The bridge pattern is meant to \"decouple an abstraction from its implementation so that the two can vary independently\", introduced by the Gang of Four. The bridge uses encapsulation, aggregation, and can use inheritance to separate responsibilities into different classes.\n\nWhen a class varies often, the features of object-oriented programming become very useful because changes to a program's code can be made easily with minimal prior knowledge about the program. The bridge pattern is useful when both the class and what it does vary often. The class itself can be thought of as the abstraction and what the class can do as the implementation. The bridge pattern can also be thought of as two layers of abstraction.\n\nThe bridge pattern is often confused with the adapter pattern, and is often implemented using the object adapter pattern. The Bridge design pattern is one of the twenty-three well-known GoF design patterns.\n\nBridge design pattern can solve the problems such as an abstraction and its implementation should be defined and extended independently from each other or a compile-time binding between an abstraction and its implementation should be avoided so that an implementation can be selected at run-time.\n\nSources & more information:\nText: Wikipedia\nen.wikipedia.org/wiki/Bridge_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Bridge_UML_class_diagram.svg",
      "item_image": "img_bridge",
      "item_favourite": false
    },

    {
      "item_name": "Builder",
      "item_summary": "A design pattern for building a complex object by using simple objects",
      "item_description": "The builder pattern is a design pattern designed to provide a flexible solution to various object creation problems in object-oriented programming. The intent of the Builder design pattern is to separate the construction of a complex object from its representation. It is one of the Gang of Four design patterns.\n\nThe Builder design pattern solves problems like: How can a class (the same construction process) create different representations of a complex object? How can a class that includes creating a complex object be simplified? Creating and assembling the parts of a complex object directly within a class is inflexible. It commits the class to creating a particular representation of the complex object and makes it impossible to change the representation later independently from (without having to change) the class.\n\nThe intent of the Builder design pattern is to separate the construction of a complex object from its representation. By doing so the same construction process can create different representations.\n\nAdvantages of the Builder pattern include: allows you to vary a product's internal representation, encapsulates code for construction and representation, provides control over steps of construction process.\n\nSources & more information:\nText:Wikipedia\nhttps://en.wikipedia.org/wiki/Builder_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Builder_UML_class_diagram.svg",
      "item_image": "img_builder",
      "item_favourite": false
    },

    {
      "item_name": "Visitor",
      "item_summary": "A design pattern which defines a new operation without changing the classes",
      "item_description": "The visitor design pattern is a way of separating an algorithm from an object structure on which it operates. A practical result of this separation is the ability to add new operations to existing object structures without modifying the structures. \n\nIn essence, the visitor allows adding new virtual functions to a family of classes, without modifying the classes. The Visitor design pattern is one of the twenty-three well-known Gang of Four design patterns.\n\nWhen new operations are needed frequently and the object structure consists of many unrelated classes, it's inflexible to add new subclasses each time a new operation is required because distributing all these operations across the various node classes leads to a system that's hard to understand, maintain, and change. Visitor pattern offers solutions by defining a separate (visitor) object that implements an operation to be performed on elements of an object structure. Clients traverse the object structure and call a dispatching operation accept(visitor) on an element — that \"dispatches\" (delegates) the request to the \"accepted visitor object\". The visitor object then performs the operation on the element (\"visits the element\"). This makes it possible to create new operations independently from the classes of an object structure by adding new visitor objects.\n\nThe nature of the Visitor makes it an ideal pattern to plug into public APIs thus allowing its clients to perform operations on a class using a \"visiting\" class without having to modify the source.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Visitor_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Visitor_design_pattern.svg",
      "item_image": "img_visitor",
      "item_favourite": false
    },

    {
      "item_name": "State",
      "item_summary": " A design pattern which allows an object to alter its behavior when its internal state changes",
      "item_description": "The state pattern is a behavioral software design pattern that allows an object to alter its behavior when its internal state changes. This pattern is close to the concept of finite-state machines. The state pattern can be interpreted as a strategy pattern, which is able to switch a strategy through invocations of methods defined in the pattern's interface.\n\nThe state pattern is used in computer programming to encapsulate varying behavior for the same object, based on its internal state. This can be a cleaner way for an object to change its behavior at runtime without resorting to conditional statements and thus improve maintainability. The state design pattern is one of twenty-three design patterns documented by the Gang of Four.\n\nThe state pattern is set to solve two main problems. An object should change its behavior when its internal state changes. State-specific behavior should be defined independently. That is, adding new states should not affect the behavior of existing states.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/State_pattern\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:State_Design_Pattern_UML_Class_Diagram.svg",
      "item_image": "img_state",
      "item_favourite": false
    }
  ]
}