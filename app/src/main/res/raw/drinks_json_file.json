{
  "drinks": [

    {
      "item_name": "Binary Search Algorithm",
      "item_summary": "A search algorithm to find the position of a target value within a sorted array",
      "item_description": "Binary search, (also known as half-interval search, logarithmic search, or binary chop) is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.\n\nBinary search runs in logarithmic time in the worst case, making {\\displaystyle O(\\log n)}O(\\log n) comparisons, where {\\displaystyle n}n is the number of elements in the array, the {\\displaystyle O}O is Big O notation, and {\\displaystyle \\log }\\log  is the logarithm.[6] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.\n\nBinary search works on sorted arrays. Binary search begins by comparing an element in the middle of the array with the target value. If the target value matches the element, its position in the array is returned. If the target value is less than the element, the search continues in the lower half of the array. If the target value is greater than the element, the search continues in the upper half of the array. By doing this, the algorithm eliminates the half in which the target value cannot lie in each iteration.\n\nIn 1946, John Mauchly made the first mention of binary search as part of the Moore School Lectures, a seminal and foundational college course in computing. In 1957, William Wesley Peterson published the first method for interpolation search. Every published binary search algorithm worked only for arrays whose length is one less than a power of two until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays.In 1962, Hermann Bottenbruch presented an ALGOL 60 implementation of binary search that placed the comparison for equality at the end, increasing the average number of iterations by one, but reducing to one the number of comparisons per iteration. The uniform binary search was developed by A. K. Chandra of Stanford University in 1971. In 1986, Bernard Chazelle and Leonidas J. Guibas introduced fractional cascading as a method to solve numerous search problems in computational geometry.\n\nFor more information: Wikipedia\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Binary_search_algorithm\nImage: Wikipedia",
      "item_image": "img_binary_search",
      "item_favourite": true
    },

    {
      "item_name": "Quick Sort Algorithm",
      "item_summary": "An efficient sorting algorithm",
      "item_description":"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm. Developed by British computer scientist Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.\n\nQuicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.\n\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.\n\nMathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.\n\nQuicksort is a divide and conquer algorithm. It first divides the input array into two smaller sub-arrays: the low elements and the high elements. It then recursively sorts the sub-arrays. \n\nFor more information: Wikipedia\n\nSource:\nImage: Wikimedia\n\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Quicksort",
      "item_image": "img_quick_sort",
      "item_favourite": false
    },

    {
      "item_name": "Insertion Sort Algorithm",
      "item_summary": "A sorting algorithm that builds the final sorted array one item at a time",
      "item_description": "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages with having simple implementation, being efficient for small data sets and being stable. When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\n\nInsertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\n\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\n\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. \n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Insertion_sort\n\nImage: Wikimedia",
      "item_image": "img_insertion_sort",
      "item_favourite": false
    },

    {
      "item_name": "Selection Sort Algorithm",
      "item_summary": "An in-place comparison sorting algorithm. ",
      "item_description": "Selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations.\n\nThe algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.\n\nThe time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort. One thing which distinguishes selection sort from other sorting algorithms is that it makes the minimum possible number of swaps, n − 1 in the worst case.\n\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of Θ(n2)), selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the kth iteration, the first k elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the k + 1st element, while selection sort must scan all remaining elements to find the k + 1st element.\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Selection_sort\nImage: Wikimedia\nhttps://upload.wikimedia.org/wikipedia/commons/6/67/SelectionSort.jpg",
      "item_image": "img_selection_sort",
      "item_favourite": false
    },

    {
      "item_name": "Bubble Sort Algorithm",
      "item_summary": "A simple sorting algorithm that repeatedly steps through the list",
      "item_description": "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements \"bubble\" to the top of the list.\n\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as merge sort are used by the sorting libraries built into popular programming languages.\n\nBubble sort has a worst-case and average complexity of О(n2), where n is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often O(n log n). Even other О(n2) sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. Therefore, bubble sort is not a practical sorting algorithm.\n\nThe only significant advantage that bubble sort has over most other algorithms, even quicksort, but not insertion sort, is that the ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only O(n). By contrast, most other algorithms, even those with better average-case complexity, perform their entire sorting process on the set and thus are more complex. However, not only does insertion sort share this advantage, but it also performs better on a list that is substantially sorted (having a small number of inversions).\n\nBubble sort should be avoided in the case of large collections. It will not be efficient in the case of a reverse-ordered collection.\n\nSources:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Bubble_sort\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:BubbleSort.jpg",
      "item_image": "img_bubble_sort",
      "item_favourite": false
    },

    {
      "item_name": "Merge Sort Algorithm",
      "item_summary": "An efficient general-purpose comparison-based sorting algorithm",
      "item_description": "Merge sort is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945.\n\nA merge sort works by dividing the unsorted list into n sublists, each containing one element (a list of one element is considered sorted) then repeatedly merging sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list. In sorting n objects, merge sort has an average and worst-case performance of O(n log n). \n\nMerge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort. Merge sort's most common implementation does not sort in place. Therefore, the memory size of the input must be allocated for the sorted output to be stored in.\n\nAlthough heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(n). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\n\nSources:\nText: Wikipedia \nhttps://en.wikipedia.org/wiki/Merge_sort\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Merge_sort_algorithm_diagram.svg ",
      "item_image": "img_merge_sort",
      "item_favourite": false
    },

    {
      "item_name": "Breadth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level. It uses the opposite strategy as depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes.\n\nBFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, but this was not published until 1972. It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze and later developed by C. Y. Lee into a wire routing algorithm which was published in 1961.\n\nBFS uses a queue and it checks whether a vertex has been discovered before enqueueing the vertex rather than delaying this check until the vertex is dequeued from the queue. The queue contains the frontier along which the algorithm is currently searching. Nodes can be labelled as discovered by storing them in a set, or by an attribute on each node, depending on the implementation. Note that the word node is usually interchangeable with the word vertex. The parent attribute of each node is useful for accessing the nodes in a shortest path, for example by backtracking from the destination node up to the starting node, once the BFS has been run, and the predecessors nodes have been set. Breadth-first search produces a so-called breadth first tree.\n\nBreadth-first search can be used to solve many problems in graph theory such as finding the shortest path between two nodes with path length measured by number of edges.\n\nSources & more info:\n\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Breadth-first_search\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Breadth-first_tree.svg",
      "item_image": "img_breadth_first_search",
      "item_favourite": false
    },

    {
      "item_name": "Depth-First Search Algorithm",
      "item_summary": "An algorithm for traversing or searching tree or graph data structures",
      "item_description": "Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.\n\nFor applications of DFS in relation to specific domains, such as searching for solutions in artificial intelligence or web-crawling, the graph to be traversed is often either too large to visit in its entirety or infinite. DFS may suffer from non-termination. In such cases, search is only performed to a limited depth; due to limited resources, such as memory or disk space, one typically does not use data structures to keep track of the set of all previously visited vertices. When an appropriate depth limit is not known a priori, iterative deepening depth-first search applies DFS repeatedly with a sequence of increasing limits. \n\nIn the artificial intelligence mode of analysis, with a branching factor greater than one, iterative deepening increases the running time by only a constant factor over the case in which the correct depth limit is known due to the geometric growth of the number of nodes per level.\n\nSources & more info:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Depth-first_search\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Depth-first-tree.svg",
      "item_image": "img_depth_first_search",
      "item_favourite": false
    },

    {
      "item_name": "Heap Sort Algorithm",
      "item_summary": "A comparison-based sorting algorithm",
      "item_description": "Heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step. Heapsort was invented by J. W. J. Williams in 1964.[2] This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort. \n\nThe heapsort algorithm can be divided into two parts. In the first step, a heap is built out of the data (see Binary heap § Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nThe Heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm. Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O(n2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. Thus, because of the O(n log n) upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort. Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort such as being a stable sort.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Heapsort\n\nImage: Wikimedia \nhttps://commons.wikimedia.org/wiki/File:Heap_sort_algorithm-phase1.svg",
      "item_image": "img_heap_sort",
      "item_favourite": false
    },

    {
      "item_name": "Kruskal's Algorithm",
      "item_summary": "A greedy algorithm to find a minimum spanning tree for a connected weighted graph.",
      "item_description": "Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. A Minimum Spanning Tree is a spanning tree of a connected, undirected graph. It connects all vertices together with the minimal weighting for its edges.\n\nThis is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component). This algorithm was written by Joseph Kruskal and first appeared in in 1956.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Kruskal's_algorithm\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Kruskal_Algorithm_1.svg",
      "item_image": "img_kruskals_algorithm",
      "item_favourite": false
    },

    {
      "item_name": "Dijkstra's Algorithm",
      "item_summary": "An algorithm for finding the shortest paths between nodes in a graph",
      "item_description": "Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\n\nThe algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.\n\nFor a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithm is network routing protocols.\n\nDijkstra's algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.\n\nSource & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\n\nImage: Wikimedia\nhttps://commons.wikimedia.org/wiki/File:Dijkstra_algorithm_example_27.svg",
      "item_image": "img_dijkstras",
      "item_favourite": false
    },

    {
      "item_name": "Stack",
      "item_summary": "A collection of elements wşth push and pop operations.",
      "item_description": "A stack is an abstract data type that serves as a collection of elements, with two principal operations. These operataions are push, which adds an element to the collection, and pop, which removes the most recently added element that was not yet removed.\n\nThe order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack.[1] The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.\n\nConsidered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack. A stack is needed to implement depth-first search.\n\nKlaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea of a stack in 1955 and filed a patent in 1957.\n\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Stack_(abstract_data_type)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Lifo_stack.png",
      "item_image": "img_stack",
      "item_favourite": false
    },

    {
      "item_name": "Queue",
      "item_summary": "A linear collection of objects that are inserted and removed according to the FIFO principle. ",
      "item_description": "A queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and removal from the other end of the sequence. By convention, the end of the sequence at which elements are added is called the back, tail, or rear of the queue and the end at which elements are removed is called the head or front of the queue, analogously to the words used when people line up to wait for goods or services.\n\nThe operation of adding an element to the rear of the queue is known as enqueue, and the operation of removing an element from the front is known as dequeue. Other operations may also be allowed, often including a peek or front operation that returns the value of the next element to be dequeued without dequeuing it.\n\nThe operations of a queue make it a first-in-first-out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. A queue is an example of a linear data structure, or more abstractly a sequential collection. Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Queue_(abstract_data_type)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Data_Queue.svg",
      "item_image": "img_queue",
      "item_favourite": false
    },

    {
      "item_name": "Graph",
      "item_summary": "A non-linear data structure consisting of nodes and edges",
      "item_description": "A graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from the field of graph theory within mathematics.\n\nA graph data structure consists of a finite (and possibly mutable) set of vertices (also called nodes or points), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as edges (also called links or lines), and for a directed graph are also known as arrows. The vertices may be part of the graph structure, or may be external entities represented by integer indices or references. A graph data structure may also associate to each edge some edge value, such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Graph_(abstract_data_type)\n\nImage: Wikipedi\nhttps://en.wikipedia.org/wiki/File:Directed.svg",
      "item_image": "img_graph",
      "item_favourite": false
    },

    {
      "item_name": "Heap",
      "item_summary": "A specialized tree-based data structure",
      "item_description": "A heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.[2] The node at the \"top\" of the heap (with no parents) is called the root node.\n\nThe heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as \"heaps\", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority.\n\nA common implementation of a heap is the binary heap, in which the tree is a binary tree. The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm.\n\nSources & more information:\nText: Wikipedia\nhttps://en.wikipedia.org/wiki/Heap_(data_structure)\n\nImage: Wikipedia\nhttps://en.wikipedia.org/wiki/File:Max-Heap.svg",
      "item_image": "img_heap",
      "item_favourite": false
    }
  ]
}